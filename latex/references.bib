@misc{resnet,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2015,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1512.03385},
	url          = {https://arxiv.org/abs/1512.03385},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{l1prune,
	title        = {Pruning Filters for Efficient ConvNets},
	author       = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
	year         = 2016,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1608.08710},
	url          = {https://arxiv.org/abs/1608.08710},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{levelpruner,
	title        = {Balanced Sparsity for Efficient {DNN} Inference on {GPU}},
	author       = {Zhuliang Yao and Shijie Cao and Wencong Xiao and Chen Zhang and Lanshun Nie},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1811.00206},
	url          = {http://arxiv.org/abs/1811.00206},
	eprinttype   = {arXiv},
	eprint       = {1811.00206},
	timestamp    = {Sun, 21 Mar 2021 17:13:43 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1811-00206.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{ray,
	title        = {Ray: A Distributed Framework for Emerging AI Applications},
	author       = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
	year         = 2017,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1712.05889},
	url          = {https://arxiv.org/abs/1712.05889},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Distributed, Parallel, and Cluster Computing (cs.DC), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{proj1-repo,
	title        = {CSC 791-025 GitHub Repository},
	howpublished = {\url{https://github.com/briancpark/csc791-025/tree/main/proj1}}
}
@misc{proj2-repo,
	title        = {CSC 791-025 GitHub Repository},
	howpublished = {\url{https://github.com/briancpark/csc791-025/tree/main/proj2}}
}
@misc{proj3-repo,
	title        = {CSC 791-025 GitHub Repository},
	howpublished = {\url{https://github.com/briancpark/csc791-025/tree/main/proj3}}
}
@software{nni,
	title        = {{Neural Network Intelligence}},
	author       = {{Microsoft}},
	year         = 2021,
	month        = 1,
	url          = {https://github.com/microsoft/nni},
	version      = {2.0}
}
@misc{arc,
	title        = {ARC: A Root Cluster for Research into Scalable Computer Systems},
	howpublished = {\url{https://arcb.csc.ncsu.edu/~mueller/cluster/arc/}}
}
@misc{m1,
	title        = {Apple Macbook Pro Specifications},
	howpublished = {\url{https://www.apple.com/macbook-pro-14-and-16/specs/}}
}
@misc{a100,
	title        = {NVIDIA A100 Datasheet},
	howpublished = {\url{https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf}}
}
@misc{v100,
	title        = {NVIDIA V100 Datasheet},
	howpublished = {\url{https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf}}
}
@misc{torchvision,
	title        = {Torchvision: Models and pre-trained weights},
	howpublished = {\url{https://pytorch.org/vision/stable/models.html}}
}
@misc{pytorchm1,
	title        = {Introducing accelerated pytorch training on Mac},
	howpublished = {\url{https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/}}
}
@article{qat,
	title        = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
	author       = {Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew G. Howard and Hartwig Adam and Dmitry Kalenichenko},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1712.05877},
	url          = {http://arxiv.org/abs/1712.05877},
	eprinttype   = {arXiv},
	eprint       = {1712.05877},
	timestamp    = {Thu, 27 May 2021 16:20:51 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1712-05877.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{dorefa,
	title        = {DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients},
	author       = {Shuchang Zhou and Zekun Ni and Xinyu Zhou and He Wen and Yuxin Wu and Yuheng Zou},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1606.06160},
	url          = {http://arxiv.org/abs/1606.06160},
	eprinttype   = {arXiv},
	eprint       = {1606.06160},
	timestamp    = {Tue, 13 Sep 2022 21:45:50 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/ZhouNZWWZ16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{binarynn,
	title        = {BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1},
	author       = {Matthieu Courbariaux and Yoshua Bengio},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1602.02830},
	url          = {http://arxiv.org/abs/1602.02830},
	eprinttype   = {arXiv},
	eprint       = {1602.02830},
	timestamp    = {Mon, 13 Aug 2018 16:46:57 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/CourbariauxB16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{lsq,
	title        = {Learned Step Size Quantization},
	author       = {Steven K. Esser and Jeffrey L. McKinstry and Deepika Bablani and Rathinakumar Appuswamy and Dharmendra S. Modha},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1902.08153},
	url          = {http://arxiv.org/abs/1902.08153},
	eprinttype   = {arXiv},
	eprint       = {1902.08153},
	timestamp    = {Tue, 21 May 2019 18:03:37 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1902-08153.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bridges-2,
	title        = {Bridges-2: A Platform for Rapidly-Evolving and Data Intensive Research},
	author       = {Brown, Shawn T. and Buitrago, Paola and Hanna, Edward and Sanielevici, Sergiu and Scibek, Robin and Nystrom, Nicholas A.},
	year         = 2021,
	booktitle    = {Practice and Experience in Advanced Research Computing},
	location     = {Boston, MA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {PEARC '21},
	doi          = {10.1145/3437359.3465593},
	isbn         = 9781450382922,
	url          = {https://doi.org/10.1145/3437359.3465593},
	abstract     = {Today’s landscape of computational science is evolving rapidly, with a need for new, flexible, and responsive supercomputing platforms for addressing the growing areas of artificial intelligence (AI), data analytics (DA) and convergent collaborative research. To support this community, we designed and deployed the Bridges-2 platform. Building on our highly successful Bridges supercomputer, which was a high-performance computing resource supporting new communities and complex workflows, Bridges-2 supports traditional and nontraditional research communities and applications; integrates new technologies for converged, scalable high-performance computing (HPC), AI, and data analytics; prioritizes researcher productivity and ease of use; and provides an extensible architecture for interoperation with complementary data intensive projects, campuses, and clouds. In this report, we describe Bridges-2’s hardware and configuration, user environments, and systems support and present the results of the successful Early User Program.},
	articleno    = 35,
	numpages     = 4,
	keywords     = {high performance computing, artificial intelligence, data analytics}
}
@inproceedings{optuna,
	title        = {Optuna: A Next-generation Hyperparameter Optimization Framework},
	author       = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	year         = 2019,
	booktitle    = {Proceedings of the 25rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining}
}
@article{ray-tune,
	title        = {Tune: {A} Research Platform for Distributed Model Selection and Training},
	author       = {Richard Liaw and Eric Liang and Robert Nishihara and Philipp Moritz and Joseph E. Gonzalez and Ion Stoica},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1807.05118},
	url          = {http://arxiv.org/abs/1807.05118},
	eprinttype   = {arXiv},
	eprint       = {1807.05118},
	timestamp    = {Mon, 13 Aug 2018 16:46:04 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1807-05118.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@online{fashion-mnist,
	title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	year         = 2017,
	date         = {2017-08-28},
	eprintclass  = {cs.LG},
	eprinttype   = {arXiv},
	eprint       = {cs.LG/1708.07747}
}
@article{cifar10,
	title        = {Learning Multiple Layers of Features from Tiny Images},
	author       = {Krizhevsky, Alex},
	year         = 2009,
	pages        = {32--33},
	url          = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
	added-at     = {2021-01-21T03:01:11.000+0100},
	biburl       = {https://www.bibsonomy.org/bibtex/2fe5248afe57647d9c85c50a98a12145c/s364315},
	interhash    = {cc2d42f2b7ef6a4e76e47d1a50c8cd86},
	intrahash    = {fe5248afe57647d9c85c50a98a12145c},
	keywords     = {},
	timestamp    = {2021-01-21T03:01:11.000+0100}
}
@inproceedings{tpe,
	title        = {Algorithms for Hyper-Parameter Optimization},
	author       = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
	year         = 2011,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 24,
	url          = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
	editor       = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
	bdsk-url-1   = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf}
}
@article{evolution,
	title        = {Large-Scale Evolution of Image Classifiers},
	author       = {Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka I. Leon{-}Suematsu and Quoc V. Le and Alex Kurakin},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1703.01041},
	url          = {http://arxiv.org/abs/1703.01041},
	eprinttype   = {arXiv},
	eprint       = {1703.01041},
	timestamp    = {Tue, 04 Oct 2022 16:44:59 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/RealMSSSLK17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{hyperband,
	title        = {Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits},
	author       = {Lisha Li and Kevin G. Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1603.06560},
	url          = {http://arxiv.org/abs/1603.06560},
	eprinttype   = {arXiv},
	eprint       = {1603.06560},
	timestamp    = {Mon, 13 Aug 2018 16:48:11 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/LiJDRT16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{vgg,
	title        = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	author       = {Simonyan, Karen and Zisserman, Andrew},
	year         = 2014,
	journal      = {CoRR},
	volume       = {abs/1409.1556},
	url          = {http://arxiv.org/abs/1409.1556},
	added-at     = {2016-11-19T13:14:27.000+0100},
	bibsource    = {dblp computer science bibliography, http://dblp.org},
	biburl       = {https://www.bibsonomy.org/bibtex/20ee0434e0a70b329d5518f43f1742f7a/albinzehe},
	interhash    = {4e6fa56cb7cf99400d5701543ee228de},
	intrahash    = {0ee0434e0a70b329d5518f43f1742f7a},
	keywords     = {cnn ma-zehe neuralnet},
	timestamp    = {2016-11-19T13:14:27.000+0100}
}
@inproceedings{hyperopt,
	title        = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
	author       = {Bergstra, James and Yamins, Daniel and Cox, David},
	year         = 2013,
	month        = {17--19 Jun},
	booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Atlanta, Georgia, USA},
	series       = {Proceedings of Machine Learning Research},
	volume       = 28,
	number       = 1,
	pages        = {115--123},
	url          = {https://proceedings.mlr.press/v28/bergstra13.html},
	editor       = {Dasgupta, Sanjoy and McAllester, David},
	pdf          = {http://proceedings.mlr.press/v28/bergstra13.pdf},
	abstract     = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method’s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.     In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included.  A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric.  Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.}
}
@inproceedings{sklearn_api,
	title        = {{API} design for machine learning software: experiences from the scikit-learn project},
	author       = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and Fabian Pedregosa and Andreas Mueller and Olivier Grisel and Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort and Jaques Grobler and Robert Layton and Jake VanderPlas and Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
	year         = 2013,
	booktitle    = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
	pages        = {108--122}
}
