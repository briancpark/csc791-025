digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4664269520 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	4664320416 [label=AddmmBackward0]
	4664320128 -> 4664320416
	4664266000 [label="fc.bias
 (1000)" fillcolor=lightblue]
	4664266000 -> 4664320128
	4664320128 [label=AccumulateGrad]
	4664321232 -> 4664320416
	4664321232 [label=ReshapeAliasBackward0]
	4664321088 -> 4664321232
	4664321088 [label=MeanBackward1]
	4664321376 -> 4664321088
	4664321376 [label=ReluBackward0]
	4664321472 -> 4664321376
	4664321472 [label=AddBackward0]
	4664321568 -> 4664321472
	4664321568 [label=NativeBatchNormBackward0]
	4664321712 -> 4664321568
	4664321712 [label=ConvolutionBackward0]
	4664321904 -> 4664321712
	4664321904 [label=ReluBackward0]
	4664322048 -> 4664321904
	4664322048 [label=NativeBatchNormBackward0]
	4664322144 -> 4664322048
	4664322144 [label=ConvolutionBackward0]
	4664321520 -> 4664322144
	4664321520 [label=ReluBackward0]
	4664322432 -> 4664321520
	4664322432 [label=AddBackward0]
	4664322528 -> 4664322432
	4664322528 [label=NativeBatchNormBackward0]
	4664318928 -> 4664322528
	4664318928 [label=ConvolutionBackward0]
	4664319072 -> 4664318928
	4664319072 [label=ReluBackward0]
	4664319120 -> 4664319072
	4664319120 [label=NativeBatchNormBackward0]
	4664319936 -> 4664319120
	4664319936 [label=ConvolutionBackward0]
	4664319648 -> 4664319936
	4664319648 [label=ReluBackward0]
	4664319456 -> 4664319648
	4664319456 [label=AddBackward0]
	4664320848 -> 4664319456
	4664320848 [label=NativeBatchNormBackward0]
	4664320560 -> 4664320848
	4664320560 [label=ConvolutionBackward0]
	4664320320 -> 4664320560
	4664320320 [label=ReluBackward0]
	4664320080 -> 4664320320
	4664320080 [label=NativeBatchNormBackward0]
	4664319168 -> 4664320080
	4664319168 [label=ConvolutionBackward0]
	4664319360 -> 4664319168
	4664319360 [label=ReluBackward0]
	4664322816 -> 4664319360
	4664322816 [label=AddBackward0]
	4664322912 -> 4664322816
	4664322912 [label=NativeBatchNormBackward0]
	4664323056 -> 4664322912
	4664323056 [label=ConvolutionBackward0]
	4664323248 -> 4664323056
	4664323248 [label=ReluBackward0]
	4664323392 -> 4664323248
	4664323392 [label=NativeBatchNormBackward0]
	4664323488 -> 4664323392
	4664323488 [label=ConvolutionBackward0]
	4664323680 -> 4664323488
	4664323680 [label=ReluBackward0]
	4664323824 -> 4664323680
	4664323824 [label=AddBackward0]
	4664323920 -> 4664323824
	4664323920 [label=NativeBatchNormBackward0]
	4664324064 -> 4664323920
	4664324064 [label=ConvolutionBackward0]
	4664324256 -> 4664324064
	4664324256 [label=ReluBackward0]
	4664324400 -> 4664324256
	4664324400 [label=NativeBatchNormBackward0]
	4664324496 -> 4664324400
	4664324496 [label=ConvolutionBackward0]
	4664323872 -> 4664324496
	4664323872 [label=ReluBackward0]
	4664324784 -> 4664323872
	4664324784 [label=AddBackward0]
	4664324880 -> 4664324784
	4664324880 [label=NativeBatchNormBackward0]
	4664325024 -> 4664324880
	4664325024 [label=ConvolutionBackward0]
	4664325216 -> 4664325024
	4664325216 [label=ReluBackward0]
	4664325360 -> 4664325216
	4664325360 [label=NativeBatchNormBackward0]
	4664325456 -> 4664325360
	4664325456 [label=ConvolutionBackward0]
	4664325648 -> 4664325456
	4664325648 [label=ReluBackward0]
	4664325792 -> 4664325648
	4664325792 [label=AddBackward0]
	4664325888 -> 4664325792
	4664325888 [label=NativeBatchNormBackward0]
	4664326032 -> 4664325888
	4664326032 [label=ConvolutionBackward0]
	4664326224 -> 4664326032
	4664326224 [label=ReluBackward0]
	4664326368 -> 4664326224
	4664326368 [label=NativeBatchNormBackward0]
	4664326464 -> 4664326368
	4664326464 [label=ConvolutionBackward0]
	4664325840 -> 4664326464
	4664325840 [label=ReluBackward0]
	4664326752 -> 4664325840
	4664326752 [label=AddBackward0]
	4664326848 -> 4664326752
	4664326848 [label=NativeBatchNormBackward0]
	4664326992 -> 4664326848
	4664326992 [label=ConvolutionBackward0]
	4664327184 -> 4664326992
	4664327184 [label=ReluBackward0]
	4664327328 -> 4664327184
	4664327328 [label=NativeBatchNormBackward0]
	4664327424 -> 4664327328
	4664327424 [label=ConvolutionBackward0]
	4664326800 -> 4664327424
	4664326800 [label=MaxPool2DWithIndicesBackward0]
	4664327712 -> 4664326800
	4664327712 [label=ReluBackward0]
	4664327808 -> 4664327712
	4664327808 [label=NativeBatchNormBackward0]
	4664327904 -> 4664327808
	4664327904 [label=ConvolutionBackward0]
	4664328096 -> 4664327904
	4664149312 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	4664149312 -> 4664328096
	4664328096 [label=AccumulateGrad]
	4664327856 -> 4664327808
	4648730384 [label="bn1.weight
 (64)" fillcolor=lightblue]
	4648730384 -> 4664327856
	4664327856 [label=AccumulateGrad]
	4664327520 -> 4664327808
	4664149392 [label="bn1.bias
 (64)" fillcolor=lightblue]
	4664149392 -> 4664327520
	4664327520 [label=AccumulateGrad]
	4664327616 -> 4664327424
	4664148272 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	4664148272 -> 4664327616
	4664327616 [label=AccumulateGrad]
	4664327376 -> 4664327328
	4664148352 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	4664148352 -> 4664327376
	4664327376 [label=AccumulateGrad]
	4664327232 -> 4664327328
	4664148192 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	4664148192 -> 4664327232
	4664327232 [label=AccumulateGrad]
	4664327136 -> 4664326992
	4664131184 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	4664131184 -> 4664327136
	4664327136 [label=AccumulateGrad]
	4664326944 -> 4664326848
	4664131264 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	4664131264 -> 4664326944
	4664326944 [label=AccumulateGrad]
	4664326896 -> 4664326848
	4664131104 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	4664131104 -> 4664326896
	4664326896 [label=AccumulateGrad]
	4664326800 -> 4664326752
	4664326656 -> 4664326464
	4664130544 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	4664130544 -> 4664326656
	4664326656 [label=AccumulateGrad]
	4664326416 -> 4664326368
	4664130624 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	4664130624 -> 4664326416
	4664326416 [label=AccumulateGrad]
	4664326272 -> 4664326368
	4664130464 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	4664130464 -> 4664326272
	4664326272 [label=AccumulateGrad]
	4664326176 -> 4664326032
	4664129664 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	4664129664 -> 4664326176
	4664326176 [label=AccumulateGrad]
	4664325984 -> 4664325888
	4664129744 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	4664129744 -> 4664325984
	4664325984 [label=AccumulateGrad]
	4664325936 -> 4664325888
	4664129584 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	4664129584 -> 4664325936
	4664325936 [label=AccumulateGrad]
	4664325840 -> 4664325792
	4664325600 -> 4664325456
	4664125824 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	4664125824 -> 4664325600
	4664325600 [label=AccumulateGrad]
	4664325408 -> 4664325360
	4664126304 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	4664126304 -> 4664325408
	4664325408 [label=AccumulateGrad]
	4664325264 -> 4664325360
	4664078032 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	4664078032 -> 4664325264
	4664325264 [label=AccumulateGrad]
	4664325168 -> 4664325024
	4664071952 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	4664071952 -> 4664325168
	4664325168 [label=AccumulateGrad]
	4664324976 -> 4664324880
	4664074112 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	4664074112 -> 4664324976
	4664324976 [label=AccumulateGrad]
	4664324928 -> 4664324880
	4664070272 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	4664070272 -> 4664324928
	4664324928 [label=AccumulateGrad]
	4664324832 -> 4664324784
	4664324832 [label=NativeBatchNormBackward0]
	4664325552 -> 4664324832
	4664325552 [label=ConvolutionBackward0]
	4664325648 -> 4664325552
	4664325696 -> 4664325552
	4664128704 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	4664128704 -> 4664325696
	4664325696 [label=AccumulateGrad]
	4664325120 -> 4664324832
	4664128624 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	4664128624 -> 4664325120
	4664325120 [label=AccumulateGrad]
	4664325072 -> 4664324832
	4664128544 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	4664128544 -> 4664325072
	4664325072 [label=AccumulateGrad]
	4664324688 -> 4664324496
	4664049344 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	4664049344 -> 4664324688
	4664324688 [label=AccumulateGrad]
	4664324448 -> 4664324400
	4664049504 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	4664049504 -> 4664324448
	4664324448 [label=AccumulateGrad]
	4664324304 -> 4664324400
	4664049184 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	4664049184 -> 4664324304
	4664324304 [label=AccumulateGrad]
	4664324208 -> 4664324064
	4664044784 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	4664044784 -> 4664324208
	4664324208 [label=AccumulateGrad]
	4664324016 -> 4664323920
	4664044944 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	4664044944 -> 4664324016
	4664324016 [label=AccumulateGrad]
	4664323968 -> 4664323920
	4664044624 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	4664044624 -> 4664323968
	4664323968 [label=AccumulateGrad]
	4664323872 -> 4664323824
	4664323632 -> 4664323488
	4664000032 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	4664000032 -> 4664323632
	4664323632 [label=AccumulateGrad]
	4664323440 -> 4664323392
	4664000192 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	4664000192 -> 4664323440
	4664323440 [label=AccumulateGrad]
	4664323296 -> 4664323392
	4663999792 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	4663999792 -> 4664323296
	4664323296 [label=AccumulateGrad]
	4664323200 -> 4664323056
	4663995632 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	4663995632 -> 4664323200
	4664323200 [label=AccumulateGrad]
	4664323008 -> 4664322912
	4663995872 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	4663995872 -> 4664323008
	4664323008 [label=AccumulateGrad]
	4664322960 -> 4664322912
	4663994912 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	4663994912 -> 4664322960
	4664322960 [label=AccumulateGrad]
	4664322864 -> 4664322816
	4664322864 [label=NativeBatchNormBackward0]
	4664323584 -> 4664322864
	4664323584 [label=ConvolutionBackward0]
	4664323680 -> 4664323584
	4664323728 -> 4664323584
	4664035504 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	4664035504 -> 4664323728
	4664323728 [label=AccumulateGrad]
	4664323152 -> 4664322864
	4664035344 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	4664035344 -> 4664323152
	4664323152 [label=AccumulateGrad]
	4664323104 -> 4664322864
	4664035104 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	4664035104 -> 4664323104
	4664323104 [label=AccumulateGrad]
	4664322720 -> 4664319168
	4663991312 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	4663991312 -> 4664322720
	4664322720 [label=AccumulateGrad]
	4664320032 -> 4664320080
	4663992672 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	4663992672 -> 4664320032
	4664320032 [label=AccumulateGrad]
	4664320272 -> 4664320080
	4663990992 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	4663990992 -> 4664320272
	4664320272 [label=AccumulateGrad]
	4664320368 -> 4664320560
	4663961344 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	4663961344 -> 4664320368
	4664320368 [label=AccumulateGrad]
	4664320752 -> 4664320848
	4663963424 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	4663963424 -> 4664320752
	4664320752 [label=AccumulateGrad]
	4664320800 -> 4664320848
	4663961184 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	4663961184 -> 4664320800
	4664320800 [label=AccumulateGrad]
	4664319360 -> 4664319456
	4664319696 -> 4664319936
	4663885584 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	4663885584 -> 4664319696
	4664319696 [label=AccumulateGrad]
	4664319984 -> 4664319120
	4663951904 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	4663951904 -> 4664319984
	4664319984 [label=AccumulateGrad]
	4664319264 -> 4664319120
	4663781760 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	4663781760 -> 4664319264
	4664319264 [label=AccumulateGrad]
	4664318832 -> 4664318928
	4664264800 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	4664264800 -> 4664318832
	4664318832 [label=AccumulateGrad]
	4664322624 -> 4664322528
	4664264880 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	4664264880 -> 4664322624
	4664322624 [label=AccumulateGrad]
	4664322576 -> 4664322528
	4664264720 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	4664264720 -> 4664322576
	4664322576 [label=AccumulateGrad]
	4664322480 -> 4664322432
	4664322480 [label=NativeBatchNormBackward0]
	4664319792 -> 4664322480
	4664319792 [label=ConvolutionBackward0]
	4664319648 -> 4664319792
	4664319600 -> 4664319792
	4663956624 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	4663956624 -> 4664319600
	4664319600 [label=AccumulateGrad]
	4664319408 -> 4664322480
	4663956464 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	4663956464 -> 4664319408
	4664319408 [label=AccumulateGrad]
	4664318880 -> 4664322480
	4663956304 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	4663956304 -> 4664318880
	4664318880 [label=AccumulateGrad]
	4664322336 -> 4664322144
	4664263600 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	4664263600 -> 4664322336
	4664322336 [label=AccumulateGrad]
	4664322096 -> 4664322048
	4664263760 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	4664263760 -> 4664322096
	4664322096 [label=AccumulateGrad]
	4664321952 -> 4664322048
	4664263360 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	4664263360 -> 4664321952
	4664321952 [label=AccumulateGrad]
	4664321856 -> 4664321712
	4664265440 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	4664265440 -> 4664321856
	4664321856 [label=AccumulateGrad]
	4664321664 -> 4664321568
	4664265360 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	4664265360 -> 4664321664
	4664321664 [label=AccumulateGrad]
	4664321616 -> 4664321568
	4664265520 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	4664265520 -> 4664321616
	4664321616 [label=AccumulateGrad]
	4664321520 -> 4664321472
	4664319312 -> 4664320416
	4664319312 [label=TBackward0]
	4664321424 -> 4664319312
	4664265920 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	4664265920 -> 4664321424
	4664321424 [label=AccumulateGrad]
	4664320416 -> 4664269520
}
